Every 0.1s: nvidia-smi                                                                                                                                            fcf2aae2e869: Wed Jan 21 09:48:41 2026

Wed Jan 21 09:48:41 2026
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           Off |   00000000:0B:01.0 Off |                    0 |
| N/A   36C    P0             54W /  300W |   24379MiB /  32768MiB |     14%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

unsloth@fcf2aae2e869:/workspace/mai-ui-trainer/trainer$ python sft_trainer.py --config configs/my_config_119.yaml 
Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.13.0
============================================================
Training Configuration Summary
============================================================
Model path: /workspace/MAI-UI-2B
Data path: /workspace/mai-ui-trainer/dataset/20260119_201327/sft_train.jsonl
Output directory: /workspace/mai-ui-trainer/trainer/models/sft_model/20260121_162232
Max length: 4096
Use LoRA: True
  LoRA r: 8
  LoRA alpha: 16
  LoRA dropout: 0.05
  LoRA target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
Use 4-bit quantization: False
Gradient checkpointing: True
============================================================

GPU compute capability (7, 0), using float16 (V100 or older)
`torch_dtype` is deprecated! Use `dtype` instead!
Loaded as Qwen3VL model with multi-modal support
VL Model: True
Gradient checkpointing enabled
trainable params: 8,716,288 || all params: 2,136,248,320 || trainable%: 0.4080
LoRA applied successfully
Loading dataset from: /workspace/mai-ui-trainer/dataset/20260119_201327/sft_train.jsonl
Dataset size: 26
Sample keys: ['prompt', 'response', 'metadata']
  prompt: You are a GUI agent. You are given a task and your action history, with screenshots. You need to perform the next action to complete the task. 

## Output Format
For each function call, return the thi...
  response: Thought: I will open the 139邮箱 app to search for the specified email and download the Excel attachment as requested.
Action: {"action": "click", "coordinate": [0.6196196196196196, 0.4984984984984985]}
  metadata: <class 'dict'>

Data contains images: True
Training with fp16=True (GPU does not support bfloat16, using float16)
Using MultiModalDataCollator for vision-language training (max 3 images/sample)

============================================================
DATASET PREPROCESSING
============================================================
  Max images per sample: 3
  Total samples: 26
  Data directory: ../dataset/20260119_201327
Decoding images:   0%|                                                                                                      | 0/26 [00:00<?, ?it/Decoding images:   4%|███▌                                                                                          | 1/26 [00:00<00:03,  6.54it/Decoding images:  12%|██████████▊                                                                                   | 3/26 [00:00<00:02, 11.26it/Decoding images:  19%|██████████████████                                                                            | 5/26 [00:00<00:01, 12.39it/Decoding images:  27%|█████████████████████████▎                                                                    | 7/26 [00:00<00:01, 10.78it/Decoding images:  35%|████████████████████████████████▌                                                             | 9/26 [00:00<00:01, 11.24it/Decoding images:  42%|███████████████████████████████████████▎                                                     | 11/26 [00:00<00:01, 11.82it/Decoding images:  50%|██████████████████████████████████████████████▌                                              | 13/26 [00:01<00:01, 12.18it/Decoding images:  58%|█████████████████████████████████████████████████████▋                                       | 15/26 [00:01<00:00, 12.54it/Decoding images:  65%|████████████████████████████████████████████████████████████▊                                | 17/26 [00:01<00:00, 12.97it/Decoding images:  73%|███████████████████████████████████████████████████████████████████▉                         | 19/26 [00:01<00:00, 11.43it/Decoding images:  81%|███████████████████████████████████████████████████████████████████████████                  | 21/26 [00:01<00:00, 12.00it/Decoding images:  88%|██████████████████████████████████████████████████████████████████████████████████▎          | 23/26 [00:01<00:00, 12.45it/Decoding images:  96%|█████████████████████████████████████████████████████████████████████████████████████████▍   | 25/26 [00:02<00:00, 12.78it/Decoding images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:02<00:00, 12.09it/s]

============================================================
PREPROCESSING SUMMARY
============================================================
  Samples processed: 26
  Original images: 26 (1.0 avg/sample)
  After limiting: 26 (1.0 avg/sample)
  Avg image size: 1080x2400 pixels
============================================================

Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
The model is already on multiple devices. Skipping the move to device specified in `args`.

NOTE: First step may be slow due to JIT compilation and image processing.
      Watch for detailed step-by-step logs below.

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.

======================================================================
TRAINING STARTED
======================================================================
  Total steps: 21
  Epochs: 3
  Batch size: 1
  Gradient accumulation: 4
  Effective batch size: 4
  Learning rate: 0.0001
  FP16: True, BF16: False
  Gradient checkpointing: True
  GPU: Tesla V100-SXM2-32GB
  GPU Memory: 31.7 GB total
======================================================================

  0%|                                                                                                                       | 0/21 [00:00<?, ?it/s]
--- Epoch 1/3 ---
[DEBUG] apply_chat_template: 0.05s, processor: 0.19s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.19s
[DEBUG] apply_chat_template: 0.00s, processor: 0.20s
[DEBUG] apply_chat_template: 0.00s, processor: 0.19s
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[Step 1/21] time: 691.71s | VRAM: 4.3/31.7GB (14%) | ETA: 3.8h
  5%|█████▏                                                                                                      | 1/21 [11:32<3:50:58, 692.94s/i                                                                                                                                                 {'loss': 1.8505, 'grad_norm': 8.668548583984375, 'learning_rate': 0.0, 'epoch': 0.15}
  5%|█████▏                                                                                                      | 1/21 [11:32<3:50:58, 692.94s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.22s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.15s
[Step 2/21] time: 702.39s | loss: 1.8505 | lr: 0.00e+00 | VRAM: 4.3/31.7GB (14%) | ETA: 3.7h
 10%|██████████▎                                                                                                 | 2/21 [23:16<3:41:21, 699.00s/i                                                                                                                                                 {'loss': 1.5209, 'grad_norm': 8.298766136169434, 'learning_rate': 2e-05, 'epoch': 0.31}
 10%|██████████▎                                                                                                 | 2/21 [23:16<3:41:21, 699.00s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.15s
[Step 3/21] time: 701.85s | loss: 1.5209 | lr: 2.00e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 3.5h
 14%|███████████████▍                                                                                            | 3/21 [34:58<3:30:12, 700.68s/i                                                                                                                                                 {'loss': 1.45, 'grad_norm': 7.575809955596924, 'learning_rate': 4e-05, 'epoch': 0.46}
 14%|███████████████▍                                                                                            | 3/21 [34:58<3:30:12, 700.68s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[Step 4/21] time: 693.01s | loss: 1.4500 | lr: 4.00e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 3.3h
 19%|████████████████████▌                                                                                       | 4/21 [46:32<3:17:45, 698.00s/i                                                                                                                                                 {'loss': 1.2761, 'grad_norm': 6.894378185272217, 'learning_rate': 6e-05, 'epoch': 0.62}
 19%|████████████████████▌                                                                                       | 4/21 [46:32<3:17:45, 698.00s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[Step 5/21] time: 703.38s | loss: 1.2761 | lr: 6.00e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 3.1h
 24%|█████████████████████████▋                                                                                  | 5/21 [58:16<3:06:43, 700.25s/i                                                                                                                                                 {'loss': 1.062, 'grad_norm': 6.077225685119629, 'learning_rate': 8e-05, 'epoch': 0.77}
 24%|█████████████████████████▋                                                                                  | 5/21 [58:16<3:06:43, 700.25s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[Step 6/21] time: 692.47s | loss: 1.0620 | lr: 8.00e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 2.9h
 29%|██████████████████████████████▎                                                                           | 6/21 [1:09:51<2:54:33, 698.24s/i                                                                                                                                                 {'loss': 0.9275, 'grad_norm': 3.545147657394409, 'learning_rate': 0.0001, 'epoch': 0.92}
 29%|██████████████████████████████▎                                                                           | 6/21 [1:09:51<2:54:33, 698.24s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[Step 7/21] time: 351.50s | loss: 0.9275 | lr: 1.00e-04 | VRAM: 4.2/31.7GB (13%) | ETA: 2.5h
 33%|███████████████████████████████████▎                                                                      | 7/21 [1:15:43<2:16:29, 584.96s/i                                                                                                                                                 {'loss': 0.8033, 'grad_norm': 4.514892578125, 'learning_rate': 9.375e-05, 'epoch': 1.0}
 33%|███████████████████████████████████▎                                                                      | 7/21 [1:15:43<2:16:29, 584.96s/it]
--- Epoch 2/3 ---
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[Step 8/21] time: 702.49s | loss: 0.8033 | lr: 9.38e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 2.4h
 38%|████████████████████████████████████████▍                                                                 | 8/21 [1:27:26<2:14:55, 622.72s/i                                                                                                                                                 {'loss': 0.6761, 'grad_norm': 2.1860458850860596, 'learning_rate': 8.75e-05, 'epoch': 1.15}
 38%|████████████████████████████████████████▍                                                                 | 8/21 [1:27:26<2:14:55, 622.72s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[Step 9/21] time: 705.12s | loss: 0.6761 | lr: 8.75e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 2.2h
 43%|█████████████████████████████████████████████▍                                                            | 9/21 [1:39:12<2:09:45, 648.77s/i                                                                                                                                                 {'loss': 0.4471, 'grad_norm': 2.337226390838623, 'learning_rate': 8.125000000000001e-05, 'epoch': 1.31}
 43%|█████████████████████████████████████████████▍                                                            | 9/21 [1:39:12<2:09:45, 648.77s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[Step 10/21] time: 689.81s | loss: 0.4471 | lr: 8.13e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 2.0h
 48%|██████████████████████████████████████████████████                                                       | 10/21 [1:50:43<2:01:18, 661.71s/i                                                                                                                                                 {'loss': 0.3887, 'grad_norm': 1.4546400308609009, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.46}
 48%|██████████████████████████████████████████████████                                                       | 10/21 [1:50:43<2:01:18, 661.71s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[Step 11/21] time: 692.77s | loss: 0.3887 | lr: 7.50e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 1.9h
 52%|███████████████████████████████████████████████████████                                                  | 11/21 [2:02:17<1:51:57, 671.74s/i                                                                                                                                                 {'loss': 0.4485, 'grad_norm': 1.650864839553833, 'learning_rate': 6.875e-05, 'epoch': 1.62}
 52%|███████████████████████████████████████████████████████                                                  | 11/21 [2:02:17<1:51:57, 671.74s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[Step 12/21] time: 693.64s | loss: 0.4485 | lr: 6.88e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 1.7h
 57%|████████████████████████████████████████████████████████████                                             | 12/21 [2:13:52<1:41:48, 678.67s/i                                                                                                                                                 {'loss': 0.4843, 'grad_norm': 1.6624587774276733, 'learning_rate': 6.25e-05, 'epoch': 1.77}
 57%|████████████████████████████████████████████████████████████                                             | 12/21 [2:13:52<1:41:48, 678.67s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.16s
[Step 13/21] time: 696.52s | loss: 0.4843 | lr: 6.25e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 1.5h
 62%|█████████████████████████████████████████████████████████████████                                        | 13/21 [2:25:29<1:31:14, 684.36s/i                                                                                                                                                 {'loss': 0.3141, 'grad_norm': 1.288277268409729, 'learning_rate': 5.6250000000000005e-05, 'epoch': 1.92}
 62%|█████████████████████████████████████████████████████████████████                                        | 13/21 [2:25:29<1:31:14, 684.36s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[Step 14/21] time: 345.65s | loss: 0.3141 | lr: 5.63e-05 | VRAM: 4.2/31.7GB (13%) | ETA: 1.3h
 67%|██████████████████████████████████████████████████████████████████████                                   | 14/21 [2:31:15<1:07:54, 582.12s/i                                                                                                                                                 {'loss': 0.3702, 'grad_norm': 1.0406144857406616, 'learning_rate': 5e-05, 'epoch': 2.0}
 67%|██████████████████████████████████████████████████████████████████████                                   | 14/21 [2:31:15<1:07:54, 582.12s/it]
--- Epoch 3/3 ---
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[Step 15/21] time: 690.81s | loss: 0.3702 | lr: 5.00e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 1.1h
 71%|███████████████████████████████████████████████████████████████████████████                              | 15/21 [2:42:47<1:01:31, 615.22s/i                                                                                                                                                 {'loss': 0.3361, 'grad_norm': 1.2179722785949707, 'learning_rate': 4.375e-05, 'epoch': 2.15}
 71%|███████████████████████████████████████████████████████████████████████████                              | 15/21 [2:42:47<1:01:31, 615.22s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[Step 16/21] time: 697.72s | loss: 0.3361 | lr: 4.37e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 54.5m
 76%|█████████████████████████████████████████████████████████████████████████████████▌                         | 16/21 [2:54:27<53:23, 640.62s/i                                                                                                                                                 {'loss': 0.33, 'grad_norm': 1.1730111837387085, 'learning_rate': 3.7500000000000003e-05, 'epoch': 2.31}
 76%|█████████████████████████████████████████████████████████████████████████████████▌                         | 16/21 [2:54:27<53:23, 640.62s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[Step 17/21] time: 695.55s | loss: 0.3300 | lr: 3.75e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 43.8m
 81%|██████████████████████████████████████████████████████████████████████████████████████▌                    | 17/21 [3:06:03<43:49, 657.41s/i                                                                                                                                                 {'loss': 0.3165, 'grad_norm': 1.0069658756256104, 'learning_rate': 3.125e-05, 'epoch': 2.46}
 81%|██████████████████████████████████████████████████████████████████████████████████████▌                    | 17/21 [3:06:03<43:49, 657.41s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.19s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[Step 18/21] time: 695.46s | loss: 0.3165 | lr: 3.13e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 32.9m
 86%|███████████████████████████████████████████████████████████████████████████████████████████▋               | 18/21 [3:17:40<33:27, 669.12s/i                                                                                                                                                 {'loss': 0.3173, 'grad_norm': 0.9751143455505371, 'learning_rate': 2.5e-05, 'epoch': 2.62}
 86%|███████████████████████████████████████████████████████████████████████████████████████████▋               | 18/21 [3:17:40<33:27, 669.12s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[Step 19/21] time: 693.65s | loss: 0.3173 | lr: 2.50e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 22.0m
 90%|████████████████████████████████████████████████████████████████████████████████████████████████▊          | 19/21 [3:29:14<22:33, 676.76s/i                                                                                                                                                 {'loss': 0.3982, 'grad_norm': 1.5680245161056519, 'learning_rate': 1.8750000000000002e-05, 'epoch': 2.77}
 90%|████████████████████████████████████████████████████████████████████████████████████████████████▊          | 19/21 [3:29:14<22:33, 676.76s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[DEBUG] apply_chat_template: 0.00s, processor: 0.19s
[DEBUG] apply_chat_template: 0.00s, processor: 0.17s
[Step 20/21] time: 707.28s | loss: 0.3982 | lr: 1.88e-05 | VRAM: 4.3/31.7GB (14%) | ETA: 11.1m
 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉     | 20/21 [3:41:02<11:26, 686.20s/i                                                                                                                                                 {'loss': 0.3268, 'grad_norm': 0.7761317491531372, 'learning_rate': 1.25e-05, 'epoch': 2.92}
 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉     | 20/21 [3:41:02<11:26, 686.20s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.18s
[Step 21/21] time: 349.23s | loss: 0.3268 | lr: 1.25e-05 | VRAM: 4.2/31.7GB (13%) | ETA: 0s
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [3:46:53<00:00, 585.39s/i                                                                                                                                                 {'loss': 0.2931, 'grad_norm': 1.0887171030044556, 'learning_rate': 6.25e-06, 'epoch': 3.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [3:46:53<00:00, 585.39s/i                                                                                                                                                 {'train_runtime': 13614.1744, 'train_samples_per_second': 0.006, 'train_steps_per_second': 0.002, 'train_loss': 0.6827329709416344, 'epoch': 3.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [3:46:54<00:00, 585.39s/it]
======================================================================
TRAINING COMPLETED
======================================================================
  Total time: 3.8h
  Total steps: 21
  Final loss: 0.2931
  Peak GPU memory: 11.23 GB
======================================================================

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [3:46:54<00:00, 648.29s/it]
LoRA weights saved to /workspace/mai-ui-trainer/trainer/models/sft_model/20260121_162232
SFT training completed. Output saved to /workspace/mai-ui-trainer/trainer/models/sft_model/20260121_162232
unsloth@fcf2aae2e869:/workspace/mai-ui-trainer/trainer$  